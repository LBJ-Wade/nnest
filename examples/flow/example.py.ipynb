{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "from getdist import plots, MCSamples\n",
    "import getdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnest.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 2\n",
    "def loglike(z):\n",
    "    z1 = z[:, 0]\n",
    "    z2 = z[:, 1]\n",
    "    return - (z1 ** 2 + z2 - 11.) ** 2 - (z1 + z2 ** 2 - 7.) ** 2\n",
    "def transform(x):\n",
    "    return 5. * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "fraction = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2 * (np.random.uniform(size=(int(n_samples / fraction), 2)) - 0.5)\n",
    "likes = loglike(transform(x))\n",
    "idx = np.argsort(-likes)\n",
    "samples = x[idx[0:n_samples]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.trainer] [INFO] SingleSpeed(\n",
      "  (net): FlowSequential(\n",
      "    (0): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[nnest.trainer] [INFO] Device [cpu]\n"
     ]
    }
   ],
   "source": [
    "t = Trainer(dims, 128,  num_blocks=5, num_layers=1, scale='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.1000]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0194] validation loss [0.0164]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0031] validation loss [-0.0015]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0024] validation loss [-0.0024]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [0.0017] validation loss [-0.0036]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [0.0015] validation loss [-0.0041]\n",
      "[nnest.trainer] [INFO] Epoch [250] train loss [0.0015] validation loss [-0.0047]\n",
      "[nnest.trainer] [INFO] Epoch [300] train loss [0.0011] validation loss [-0.0050]\n",
      "[nnest.trainer] [INFO] Epoch [350] train loss [0.0008] validation loss [-0.0053]\n",
      "[nnest.trainer] [INFO] Epoch [400] train loss [0.0000] validation loss [-0.0054]\n",
      "[nnest.trainer] [INFO] Epoch [450] train loss [0.0001] validation loss [-0.0057]\n",
      "[nnest.trainer] [INFO] Epoch [500] train loss [0.0000] validation loss [-0.0059]\n",
      "[nnest.trainer] [INFO] Epoch [502] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [452] validation loss [-0.0068]\n"
     ]
    }
   ],
   "source": [
    "t.train(samples, jitter=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_synth = t.netG.sample(samples.size).detach().cpu().numpy()\n",
    "z = t.get_latent_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = []\n",
    "for x in np.linspace(np.min(samples[:, 0])*0.8, np.max(samples[:, 0])*1.2, 10):\n",
    "    for y in np.linspace(np.min(samples[:, 1])*0.8, np.max(samples[:, 1])*1.2, 5000):\n",
    "        grid.append([x, y])\n",
    "for y in np.linspace(np.min(samples[:, 1])*0.8, np.max(samples[:, 1])*1.2, 10):\n",
    "    for x in np.linspace(np.min(samples[:, 0])*0.8, np.max(samples[:, 0])*1.2, 5000):\n",
    "        grid.append([x, y])\n",
    "grid = np.array(grid)\n",
    "z_grid = t.get_latent_samples(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ppzam/anaconda/envs/flows/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 5))\n",
    "ax[0].scatter(grid[:, 0], grid[:, 1], c=grid[:, 0], marker='.', s=1, linewidths=0)\n",
    "ax[0].scatter(samples[:, 0], samples[:, 1], s=4)\n",
    "ax[0].set_title('Real data')\n",
    "ax[1].scatter(z_grid[:, 0], z_grid[:, 1], c=grid[:, 0], marker='.', s=1, linewidths=0)\n",
    "ax[1].scatter(z[:, 0], z[:, 1], s=4)\n",
    "ax[1].set_title('Latent data')\n",
    "ax[1].set_xlim([-np.max(np.abs(z)), np.max(np.abs(z))])\n",
    "ax[1].set_ylim([-np.max(np.abs(z)), np.max(np.abs(z))])\n",
    "ax[2].scatter(x_synth[:, 0], x_synth[:, 1], s=2)\n",
    "ax[2].set_title('Synthetic data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
