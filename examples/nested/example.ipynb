{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "from getdist import plots, MCSamples\n",
    "import getdist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnest import NestedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Himmelblau\n",
    "def loglike(z):\n",
    "    z1 = z[:, 0]\n",
    "    z2 = z[:, 1]\n",
    "    return - (z1**2 + z2 - 11.)**2 - (z1 + z2**2 - 7.)**2\n",
    "def transform(x):\n",
    "    return 5. * x\n",
    "dims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock\n",
    "def loglike(z):\n",
    "    return np.array([-sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0) for x in z])\n",
    "def transform(x):\n",
    "    return 5. * x\n",
    "dims = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for new run logs/test/run19\n",
      "[nnest.trainer] [INFO] SingleSpeed(\n",
      "  (net): FlowSequential(\n",
      "    (0): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): CouplingLayer(\n",
      "      (scale_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): Tanh()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "      (translate_net): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[nnest.trainer] [INFO] Device [cpu]\n",
      "[nnest.sampler] [INFO] Num base params [2]\n",
      "[nnest.sampler] [INFO] Num derived params [0]\n",
      "[nnest.sampler] [INFO] Total params [2]\n",
      "[nnest.sampler] [INFO] Num live points [1000]\n"
     ]
    }
   ],
   "source": [
    "sampler = NestedSampler(dims, loglike, transform=transform, num_live_points=1000, hidden_dim=128, num_layers=1, num_blocks=5, scale='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.sampler] [INFO] MCMC steps [10]\n",
      "[nnest.sampler] [INFO] Initial scale [1.4142]\n",
      "[nnest.sampler] [INFO] Volume switch [-1.0000]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0032]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0200] validation loss [0.0191]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0152] validation loss [0.0161]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0149] validation loss [0.0156]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [0.0148] validation loss [0.0159]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [0.0145] validation loss [0.0157]\n",
      "[nnest.trainer] [INFO] Epoch [250] train loss [0.0145] validation loss [0.0157]\n",
      "[nnest.trainer] [INFO] Epoch [300] train loss [0.0145] validation loss [0.0156]\n",
      "[nnest.trainer] [INFO] Epoch [322] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [272] validation loss [0.0154]\n",
      "[nnest.sampler] [INFO] Step [0] loglstar [-8.6684e+04] max logl [-7.1632e-02] logz [-8.6691e+04] vol [1.00000e+00] ncalls [1001] mean calls [1.0000]\n",
      "[nnest.sampler] [INFO] Step [200] loglstar [-2.8742e+04] max logl [-7.1632e-02] logz [-2.8749e+04] vol [8.18731e-01] ncalls [1217] mean calls [1.4000]\n",
      "[nnest.sampler] [INFO] Step [400] loglstar [-1.3554e+04] max logl [-7.1632e-02] logz [-1.3561e+04] vol [6.70320e-01] ncalls [1487] mean calls [1.1000]\n",
      "[nnest.sampler] [INFO] Step [600] loglstar [-6.3384e+03] max logl [-1.8491e-02] logz [-6.3459e+03] vol [5.48812e-01] ncalls [1784] mean calls [1.8000]\n",
      "[nnest.sampler] [INFO] Step [800] loglstar [-2.9639e+03] max logl [-1.8491e-02] logz [-2.9717e+03] vol [4.49329e-01] ncalls [2182] mean calls [2.0000]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0019]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0120] validation loss [0.0083]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0048] validation loss [0.0051]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0047] validation loss [0.0051]\n",
      "[nnest.trainer] [INFO] Epoch [133] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [83] validation loss [0.0049]\n",
      "[nnest.sampler] [INFO] Step [1000] loglstar [-1.7919e+03] max logl [-1.8491e-02] logz [-1.7994e+03] vol [3.67879e-01] ncalls [2664] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [1200] loglstar [-1.1737e+03] max logl [-1.8491e-02] logz [-1.1818e+03] vol [3.01194e-01] ncalls [3225] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [1400] loglstar [-7.9638e+02] max logl [-1.8491e-02] logz [-8.0419e+02] vol [2.46597e-01] ncalls [3941] mean calls [3.3000]\n",
      "[nnest.sampler] [INFO] Step [1600] loglstar [-5.1999e+02] max logl [-1.8491e-02] logz [-5.2780e+02] vol [2.01897e-01] ncalls [4813] mean calls [7.5000]\n",
      "[nnest.sampler] [INFO] Switching to MCMC sampling\n",
      "[nnest.sampler] [INFO] Acceptance [0.5100] min ESS [3.5147] max ESS [4.8512] average jump distance [0.0891]\n",
      "[nnest.sampler] [INFO] Step [1800] loglstar [-3.3824e+02] maxlogl [-1.8491e-02] logz [-3.4620e+02] vol [1.65299e-01] ncalls [6284] scale [0.6249]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0011]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0023] validation loss [-0.0002]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0050] validation loss [-0.0044]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0053] validation loss [-0.0048]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0051] validation loss [-0.0048]\n",
      "[nnest.trainer] [INFO] Epoch [168] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [118] validation loss [-0.0050]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6000] min ESS [2.6775] max ESS [3.0504] average jump distance [0.0863]\n",
      "[nnest.sampler] [INFO] Step [2000] loglstar [-2.1901e+02] maxlogl [-4.9647e-03] logz [-2.2703e+02] vol [1.35335e-01] ncalls [7867] scale [0.3002]\n",
      "[nnest.sampler] [INFO] Acceptance [0.4500] min ESS [3.8672] max ESS [5.5876] average jump distance [0.1320]\n",
      "[nnest.sampler] [INFO] Step [2200] loglstar [-1.4712e+02] maxlogl [-4.9647e-03] logz [-1.5508e+02] vol [1.10803e-01] ncalls [9133] scale [0.7633]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6000] min ESS [3.0066] max ESS [4.8230] average jump distance [0.1149]\n",
      "[nnest.sampler] [INFO] Step [2400] loglstar [-9.7240e+01] maxlogl [-4.9647e-03] logz [-1.0504e+02] vol [9.07180e-02] ncalls [10477] scale [0.8159]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5700] min ESS [2.2022] max ESS [2.5009] average jump distance [0.0864]\n",
      "[nnest.sampler] [INFO] Step [2600] loglstar [-6.6875e+01] maxlogl [-4.9647e-03] logz [-7.4038e+01] vol [7.42736e-02] ncalls [12053] scale [0.6907]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6500] min ESS [3.1078] max ESS [4.2309] average jump distance [0.0691]\n",
      "[nnest.sampler] [INFO] Step [2800] loglstar [-4.6135e+01] maxlogl [-4.9647e-03] logz [-5.3154e+01] vol [6.08101e-02] ncalls [13587] scale [0.3463]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0007]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0077] validation loss [-0.0095]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0143] validation loss [-0.0139]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0143] validation loss [-0.0145]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0145] validation loss [-0.0140]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.0147] validation loss [-0.0149]\n",
      "[nnest.trainer] [INFO] Epoch [202] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [152] validation loss [-0.0150]\n",
      "[nnest.sampler] [INFO] Acceptance [0.4800] min ESS [2.3265] max ESS [3.0808] average jump distance [0.0682]\n",
      "[nnest.sampler] [INFO] Step [3000] loglstar [-3.1652e+01] maxlogl [-4.9647e-03] logz [-3.8840e+01] vol [4.97871e-02] ncalls [15201] scale [0.2808]\n",
      "[nnest.sampler] [INFO] Acceptance [0.4400] min ESS [4.2444] max ESS [5.6465] average jump distance [0.1536]\n",
      "[nnest.sampler] [INFO] Step [3200] loglstar [-2.2613e+01] maxlogl [-3.6044e-03] logz [-2.9471e+01] vol [4.07622e-02] ncalls [16444] scale [0.4630]\n",
      "[nnest.sampler] [INFO] Acceptance [0.4700] min ESS [4.6605] max ESS [5.4192] average jump distance [0.1154]\n",
      "[nnest.sampler] [INFO] Step [3400] loglstar [-1.6013e+01] maxlogl [-3.6044e-03] logz [-2.2779e+01] vol [3.33733e-02] ncalls [17837] scale [0.5601]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6500] min ESS [3.8075] max ESS [4.4247] average jump distance [0.0860]\n",
      "[nnest.sampler] [INFO] Step [3600] loglstar [-1.1568e+01] maxlogl [-3.6044e-03] logz [-1.8095e+01] vol [2.73237e-02] ncalls [19399] scale [0.6744]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6200] min ESS [3.9463] max ESS [5.8496] average jump distance [0.0867]\n",
      "[nnest.sampler] [INFO] Step [3800] loglstar [-9.1817e+00] maxlogl [-3.6044e-03] logz [-1.5396e+01] vol [2.23708e-02] ncalls [21006] scale [0.6907]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0004]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0173] validation loss [-0.0190]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0039] validation loss [-0.0054]\n",
      "[nnest.trainer] [INFO] Epoch [95] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [45] validation loss [-0.0251]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5400] min ESS [2.4703] max ESS [3.5348] average jump distance [0.0672]\n",
      "[nnest.sampler] [INFO] Step [4000] loglstar [-6.9610e+00] maxlogl [-1.4850e-04] logz [-1.3294e+01] vol [1.83156e-02] ncalls [22693] scale [0.4189]\n",
      "[nnest.sampler] [INFO] Acceptance [0.4900] min ESS [8.3517] max ESS [8.4001] average jump distance [0.1206]\n",
      "[nnest.sampler] [INFO] Step [4200] loglstar [-5.5001e+00] maxlogl [-1.4850e-04] logz [-1.1663e+01] vol [1.49956e-02] ncalls [24037] scale [0.8722]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5800] min ESS [5.5705] max ESS [9.9494] average jump distance [0.1086]\n",
      "[nnest.sampler] [INFO] Step [4400] loglstar [-4.4078e+00] maxlogl [-1.4850e-04] logz [-1.0472e+01] vol [1.22773e-02] ncalls [25529] scale [0.5379]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5100] min ESS [2.8767] max ESS [4.4001] average jump distance [0.0672]\n",
      "[nnest.sampler] [INFO] Step [4600] loglstar [-3.5194e+00] maxlogl [-1.4850e-04] logz [-9.5227e+00] vol [1.00518e-02] ncalls [27065] scale [0.7507]\n",
      "[nnest.sampler] [INFO] Acceptance [0.7000] min ESS [5.3429] max ESS [7.2728] average jump distance [0.0807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.sampler] [INFO] Step [4800] loglstar [-2.7094e+00] maxlogl [-1.4850e-04] logz [-8.7652e+00] vol [8.22975e-03] ncalls [28580] scale [0.7330]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0003]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0268] validation loss [-0.0275]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0335] validation loss [-0.0333]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0340] validation loss [-0.0335]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0341] validation loss [-0.0343]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.0345] validation loss [-0.0342]\n",
      "[nnest.trainer] [INFO] Epoch [250] train loss [-0.0346] validation loss [-0.0343]\n",
      "[nnest.trainer] [INFO] Epoch [300] train loss [-0.0345] validation loss [-0.0345]\n",
      "[nnest.trainer] [INFO] Epoch [350] train loss [-0.0342] validation loss [-0.0339]\n",
      "[nnest.trainer] [INFO] Epoch [358] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [308] validation loss [-0.0346]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5700] min ESS [4.3862] max ESS [4.8278] average jump distance [0.0377]\n",
      "[nnest.sampler] [INFO] Step [5000] loglstar [-2.1541e+00] maxlogl [-1.4850e-04] logz [-8.1491e+00] vol [6.73795e-03] ncalls [30199] scale [0.6570]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5600] min ESS [3.2269] max ESS [5.9179] average jump distance [0.0704]\n",
      "[nnest.sampler] [INFO] Step [5200] loglstar [-1.7311e+00] maxlogl [-1.4850e-04] logz [-7.6739e+00] vol [5.51656e-03] ncalls [31577] scale [0.5379]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6500] min ESS [7.5481] max ESS [9.0046] average jump distance [0.0965]\n",
      "[nnest.sampler] [INFO] Step [5400] loglstar [-1.4043e+00] maxlogl [-1.4850e-04] logz [-7.2989e+00] vol [4.51658e-03] ncalls [32941] scale [1.1387]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5800] min ESS [6.4414] max ESS [8.6477] average jump distance [0.0880]\n",
      "[nnest.sampler] [INFO] Step [5600] loglstar [-1.1254e+00] maxlogl [-1.4850e-04] logz [-7.0042e+00] vol [3.69786e-03] ncalls [34336] scale [0.6461]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6200] min ESS [5.2627] max ESS [9.9308] average jump distance [0.0718]\n",
      "[nnest.sampler] [INFO] Step [5800] loglstar [-9.3582e-01] maxlogl [-1.4850e-04] logz [-6.7700e+00] vol [3.02755e-03] ncalls [35827] scale [0.5379]\n",
      "[nnest.trainer] [INFO] Number of training samples [1000]\n",
      "[nnest.trainer] [INFO] Training noise [0.0002]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0385] validation loss [-0.0395]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0442] validation loss [-0.0417]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0419] validation loss [-0.0397]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0439] validation loss [-0.0442]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.0446] validation loss [-0.0442]\n",
      "[nnest.trainer] [INFO] Epoch [250] train loss [-0.0439] validation loss [-0.0432]\n",
      "[nnest.trainer] [INFO] Epoch [257] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [207] validation loss [-0.0443]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5600] min ESS [7.8981] max ESS [10.0777] average jump distance [0.0735]\n",
      "[nnest.sampler] [INFO] Step [6000] loglstar [-7.5480e-01] maxlogl [-1.4850e-04] logz [-6.5824e+00] vol [2.47875e-03] ncalls [37510] scale [0.4553]\n",
      "[nnest.sampler] [INFO] Acceptance [0.7300] min ESS [7.6719] max ESS [9.9755] average jump distance [0.0733]\n",
      "[nnest.sampler] [INFO] Step [6200] loglstar [-6.2675e-01] maxlogl [-1.4850e-04] logz [-6.4321e+00] vol [2.02943e-03] ncalls [38855] scale [1.0666]\n",
      "[nnest.sampler] [INFO] Acceptance [0.6700] min ESS [7.3391] max ESS [10.0111] average jump distance [0.0644]\n",
      "[nnest.sampler] [INFO] Step [6400] loglstar [-5.1725e-01] maxlogl [-1.4850e-04] logz [-6.3109e+00] vol [1.66156e-03] ncalls [40256] scale [1.0858]\n",
      "[nnest.sampler] [INFO] Acceptance [0.5500] min ESS [6.8401] max ESS [9.9298] average jump distance [0.0478]\n",
      "[nnest.sampler] [INFO] Step [6600] loglstar [-4.2393e-01] maxlogl [-1.4850e-04] logz [-6.2125e+00] vol [1.36037e-03] ncalls [41704] scale [0.5379]\n",
      "niter: 6633\n",
      " ncall: 41937\n",
      " nsamples: 7633\n",
      " logz: -5.771 +/-  0.070\n",
      " h:  4.848\n"
     ]
    }
   ],
   "source": [
    "sampler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.770625395179834\n"
     ]
    }
   ],
   "source": [
    "print(sampler.logz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed no burn in\n"
     ]
    }
   ],
   "source": [
    "mc = MCSamples(samples=sampler.samples, weights=sampler.weights, loglikes=sampler.loglikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginalized limits: 0.68; 0.95; 0.99\n",
      "\n",
      "parameter   mean           sddev          lower1         upper1         limit1 lower2         upper2         limit2 lower3         upper3         limit3 \n",
      "param1      9.2350237E-01  7.5681498E-01  6.0552437E-02  1.9388670E+00  two   -4.0960681E-01  2.1903465E+00  two   -8.2789581E-01  2.3536998E+00  two     p_{1}\n",
      "param2      1.4242261E+00  1.4103023E+00 -1.2103676E-01  3.1321822E+00  two   -1.5050935E-01  4.2480386E+00  two   -1.7944064E-01  4.8878672E+00  two     p_{2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mc.getMargeStats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.getSubplotPlotter(width_inch=8)\n",
    "g.triangle_plot(mc, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1974.172407438173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.getEffectiveSamples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
