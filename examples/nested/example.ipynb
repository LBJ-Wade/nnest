{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example notebook showing how to use the nested sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "from getdist import plots, MCSamples\n",
    "import getdist\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.realpath(os.path.join(os.getcwd(), '../..'))\n",
    "sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnest import NestedSampler\n",
    "from nnest.likelihoods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood + prior\n",
    "#like = Himmelblau(2)\n",
    "#transform = lambda x: 5*x\n",
    "like = Rosenbrock(10)\n",
    "transform = lambda x: 5*x\n",
    "#like = Gaussian(2, 0.9)\n",
    "#transform = lambda x: 3*x\n",
    "#like = Eggbox(2)\n",
    "#transform = lambda x: 5*np.pi*x\n",
    "#like = GaussianShell(2)\n",
    "#transform = lambda x: 5*x\n",
    "#like = GaussianMix(2)\n",
    "#transform = lambda x: 5*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for new run logs/test/run50\n",
      "[nnest.trainer] [INFO] SingleSpeedSpline(\n",
      "  (flow): NormalizingFlow(\n",
      "    (flows): ModuleList(\n",
      "      (0): ActNorm()\n",
      "      (1): Invertible1x1Conv()\n",
      "      (2): NSF_CL(\n",
      "        (f1): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (f2): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ActNorm()\n",
      "      (4): Invertible1x1Conv()\n",
      "      (5): NSF_CL(\n",
      "        (f1): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (f2): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): ActNorm()\n",
      "      (7): Invertible1x1Conv()\n",
      "      (8): NSF_CL(\n",
      "        (f1): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (f2): MLP(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=5, out_features=16, bias=True)\n",
      "            (1): LeakyReLU(negative_slope=0.2)\n",
      "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (3): LeakyReLU(negative_slope=0.2)\n",
      "            (4): Linear(in_features=16, out_features=16, bias=True)\n",
      "            (5): LeakyReLU(negative_slope=0.2)\n",
      "            (6): Linear(in_features=16, out_features=115, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[nnest.trainer] [INFO] Number of network params: [16260]\n",
      "[nnest.trainer] [INFO] Device [cpu]\n",
      "[nnest.sampler] [INFO] Num base params [10]\n",
      "[nnest.sampler] [INFO] Num derived params [0]\n",
      "[nnest.sampler] [INFO] Total params [10]\n",
      "[nnest.sampler] [INFO] Num live points [2000]\n"
     ]
    }
   ],
   "source": [
    "sampler = NestedSampler(like.x_dim, like, transform=transform, num_live_points=2000, hidden_dim=16, num_blocks=3, flow='spline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.sampler] [INFO] MCMC steps [50]\n",
      "[nnest.sampler] [INFO] Initial scale [0.6325]\n",
      "[nnest.sampler] [INFO] Volume switch [0.1000]\n",
      "[nnest.sampler] [INFO] Step [0] loglstar [-3.2505e+05] max logl [-6.0433e+03] logz [-3.2506e+05] vol [1.00000e+00] ncalls [2001] mean calls [0.0000]\n",
      "[nnest.sampler] [INFO] Step [400] loglstar [-1.6843e+05] max logl [-6.0433e+03] logz [-1.6844e+05] vol [8.18731e-01] ncalls [2454] mean calls [1.2000]\n",
      "[nnest.sampler] [INFO] Step [800] loglstar [-1.4042e+05] max logl [-6.0433e+03] logz [-1.4043e+05] vol [6.70320e-01] ncalls [2974] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [1200] loglstar [-1.2028e+05] max logl [-6.0433e+03] logz [-1.2029e+05] vol [5.48812e-01] ncalls [3609] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [1600] loglstar [-1.0717e+05] max logl [-6.0433e+03] logz [-1.0718e+05] vol [4.49329e-01] ncalls [4431] mean calls [2.5000]\n",
      "[nnest.sampler] [INFO] Step [2000] loglstar [-9.6120e+04] max logl [-6.0433e+03] logz [-9.6128e+04] vol [3.67879e-01] ncalls [5468] mean calls [3.5000]\n",
      "[nnest.sampler] [INFO] Step [2400] loglstar [-8.7899e+04] max logl [-6.0433e+03] logz [-8.7908e+04] vol [3.01194e-01] ncalls [6739] mean calls [3.0000]\n",
      "[nnest.sampler] [INFO] Step [2800] loglstar [-7.9707e+04] max logl [-5.5033e+03] logz [-7.9716e+04] vol [2.46597e-01] ncalls [8227] mean calls [4.2000]\n",
      "[nnest.sampler] [INFO] Step [3200] loglstar [-7.2986e+04] max logl [-3.4648e+03] logz [-7.2996e+04] vol [2.01897e-01] ncalls [9909] mean calls [6.0000]\n",
      "[nnest.sampler] [INFO] Step [3600] loglstar [-6.6743e+04] max logl [-1.4350e+03] logz [-6.6752e+04] vol [1.65299e-01] ncalls [11969] mean calls [5.4000]\n",
      "[nnest.sampler] [INFO] Step [4000] loglstar [-6.1091e+04] max logl [-1.4350e+03] logz [-6.1100e+04] vol [1.35335e-01] ncalls [14602] mean calls [10.7000]\n",
      "[nnest.sampler] [INFO] Step [4400] loglstar [-5.6074e+04] max logl [-1.4350e+03] logz [-5.6083e+04] vol [1.10803e-01] ncalls [18074] mean calls [14.0000]\n",
      "[nnest.sampler] [INFO] Rejection prior no longer efficient\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0719]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0687] validation loss [0.0339]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0596] validation loss [0.0299]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0582] validation loss [0.0294]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [0.0572] validation loss [0.0292]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [0.0563] validation loss [0.0291]\n",
      "[nnest.trainer] [INFO] Epoch [245] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [195] validation loss [0.0290]\n",
      "[nnest.sampler] [INFO] Step [4800] loglstar [-5.1617e+04] max logl [-1.3970e+03] logz [-5.1627e+04] vol [9.07180e-02] ncalls [20230] mean calls [1.6000]\n",
      "[nnest.sampler] [INFO] Step [5200] loglstar [-4.6444e+04] max logl [-1.3970e+03] logz [-4.6454e+04] vol [7.42736e-02] ncalls [20828] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [5600] loglstar [-4.1685e+04] max logl [-1.3970e+03] logz [-4.1695e+04] vol [6.08101e-02] ncalls [21455] mean calls [1.3000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0637]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0466] validation loss [0.0222]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0435] validation loss [0.0213]\n",
      "[nnest.trainer] [INFO] Epoch [65] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [15] validation loss [0.0212]\n",
      "[nnest.sampler] [INFO] Step [6000] loglstar [-3.6922e+04] max logl [-1.2021e+03] logz [-3.6933e+04] vol [4.97871e-02] ncalls [22212] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [6400] loglstar [-3.2695e+04] max logl [-9.0792e+02] logz [-3.2706e+04] vol [4.07622e-02] ncalls [22800] mean calls [1.3000]\n",
      "[nnest.sampler] [INFO] Step [6800] loglstar [-2.8440e+04] max logl [-9.0792e+02] logz [-2.8451e+04] vol [3.33733e-02] ncalls [23493] mean calls [1.6000]\n",
      "[nnest.sampler] [INFO] Step [7200] loglstar [-2.4496e+04] max logl [-9.0792e+02] logz [-2.4507e+04] vol [2.73237e-02] ncalls [24258] mean calls [2.4000]\n",
      "[nnest.sampler] [INFO] Step [7600] loglstar [-2.1625e+04] max logl [-9.0792e+02] logz [-2.1636e+04] vol [2.23708e-02] ncalls [25182] mean calls [2.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0528]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0311] validation loss [0.0142]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0254] validation loss [0.0125]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0248] validation loss [0.0125]\n",
      "[nnest.trainer] [INFO] Epoch [141] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [91] validation loss [0.0125]\n",
      "[nnest.sampler] [INFO] Step [8000] loglstar [-1.8977e+04] max logl [-9.0792e+02] logz [-1.8989e+04] vol [1.83156e-02] ncalls [26210] mean calls [2.5000]\n",
      "[nnest.sampler] [INFO] Step [8400] loglstar [-1.6436e+04] max logl [-5.7949e+02] logz [-1.6447e+04] vol [1.49956e-02] ncalls [26792] mean calls [1.6000]\n",
      "[nnest.sampler] [INFO] Step [8800] loglstar [-1.4198e+04] max logl [-2.1628e+02] logz [-1.4210e+04] vol [1.22773e-02] ncalls [27465] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [9200] loglstar [-1.2331e+04] max logl [-2.1628e+02] logz [-1.2343e+04] vol [1.00518e-02] ncalls [28213] mean calls [2.4000]\n",
      "[nnest.sampler] [INFO] Step [9600] loglstar [-1.0716e+04] max logl [-2.1628e+02] logz [-1.0728e+04] vol [8.22975e-03] ncalls [29098] mean calls [1.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0449]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [0.0134] validation loss [0.0059]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [0.0082] validation loss [0.0047]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [0.0077] validation loss [0.0047]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [0.0068] validation loss [0.0046]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [0.0065] validation loss [0.0046]\n",
      "[nnest.trainer] [INFO] Epoch [206] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [156] validation loss [0.0046]\n",
      "[nnest.sampler] [INFO] Step [10000] loglstar [-9.3999e+03] max logl [-2.1628e+02] logz [-9.4122e+03] vol [6.73795e-03] ncalls [30124] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [10400] loglstar [-8.2790e+03] max logl [-2.1628e+02] logz [-8.2918e+03] vol [5.51656e-03] ncalls [30722] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [10800] loglstar [-7.1436e+03] max logl [-2.1628e+02] logz [-7.1559e+03] vol [4.51658e-03] ncalls [31351] mean calls [1.6000]\n",
      "[nnest.sampler] [INFO] Step [11200] loglstar [-6.3353e+03] max logl [-2.1628e+02] logz [-6.3480e+03] vol [3.69786e-03] ncalls [32112] mean calls [2.2000]\n",
      "[nnest.sampler] [INFO] Step [11600] loglstar [-5.6413e+03] max logl [-2.1628e+02] logz [-5.6539e+03] vol [3.02755e-03] ncalls [33006] mean calls [2.4000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0385]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0029] validation loss [-0.0030]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0078] validation loss [-0.0040]\n",
      "[nnest.trainer] [INFO] Epoch [93] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [43] validation loss [-0.0040]\n",
      "[nnest.sampler] [INFO] Step [12000] loglstar [-4.9997e+03] max logl [-2.1628e+02] logz [-5.0121e+03] vol [2.47875e-03] ncalls [34039] mean calls [2.2000]\n",
      "[nnest.sampler] [INFO] Step [12400] loglstar [-4.4775e+03] max logl [-2.1628e+02] logz [-4.4906e+03] vol [2.02943e-03] ncalls [34645] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [12800] loglstar [-3.9830e+03] max logl [-2.1628e+02] logz [-3.9963e+03] vol [1.66156e-03] ncalls [35348] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [13200] loglstar [-3.5490e+03] max logl [-2.1628e+02] logz [-3.5626e+03] vol [1.36037e-03] ncalls [36111] mean calls [1.4000]\n",
      "[nnest.sampler] [INFO] Step [13600] loglstar [-3.1794e+03] max logl [-2.1628e+02] logz [-3.1936e+03] vol [1.11378e-03] ncalls [37055] mean calls [1.9000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0333]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0171] validation loss [-0.0105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0221] validation loss [-0.0118]\n",
      "[nnest.trainer] [INFO] Epoch [74] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [24] validation loss [-0.0119]\n",
      "[nnest.sampler] [INFO] Step [14000] loglstar [-2.8812e+03] max logl [-2.1628e+02] logz [-2.8953e+03] vol [9.11882e-04] ncalls [38130] mean calls [3.8000]\n",
      "[nnest.sampler] [INFO] Step [14400] loglstar [-2.5736e+03] max logl [-1.9390e+02] logz [-2.5875e+03] vol [7.46586e-04] ncalls [38763] mean calls [1.8000]\n",
      "[nnest.sampler] [INFO] Step [14800] loglstar [-2.3106e+03] max logl [-1.9390e+02] logz [-2.3252e+03] vol [6.11253e-04] ncalls [39523] mean calls [2.0000]\n",
      "[nnest.sampler] [INFO] Step [15200] loglstar [-2.0970e+03] max logl [-1.9390e+02] logz [-2.1106e+03] vol [5.00451e-04] ncalls [40416] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [15600] loglstar [-1.8795e+03] max logl [-1.9390e+02] logz [-1.8937e+03] vol [4.09735e-04] ncalls [41450] mean calls [3.4000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0289]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0310] validation loss [-0.0172]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0357] validation loss [-0.0185]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0364] validation loss [-0.0186]\n",
      "[nnest.trainer] [INFO] Epoch [148] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [98] validation loss [-0.0187]\n",
      "[nnest.sampler] [INFO] Step [16000] loglstar [-1.7074e+03] max logl [-1.9390e+02] logz [-1.7218e+03] vol [3.35463e-04] ncalls [42632] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [16400] loglstar [-1.5501e+03] max logl [-1.9390e+02] logz [-1.5649e+03] vol [2.74654e-04] ncalls [43283] mean calls [1.6000]\n",
      "[nnest.sampler] [INFO] Step [16800] loglstar [-1.4127e+03] max logl [-1.7624e+02] logz [-1.4274e+03] vol [2.24867e-04] ncalls [44033] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [17200] loglstar [-1.2963e+03] max logl [-1.0483e+02] logz [-1.3105e+03] vol [1.84106e-04] ncalls [44857] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [17600] loglstar [-1.1846e+03] max logl [-1.0483e+02] logz [-1.1997e+03] vol [1.50733e-04] ncalls [45818] mean calls [1.9000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0253]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0454] validation loss [-0.0245]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0498] validation loss [-0.0258]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0498] validation loss [-0.0260]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0510] validation loss [-0.0261]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.0510] validation loss [-0.0261]\n",
      "[nnest.trainer] [INFO] Epoch [248] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [198] validation loss [-0.0262]\n",
      "[nnest.sampler] [INFO] Step [18000] loglstar [-1.0850e+03] max logl [-1.0483e+02] logz [-1.0999e+03] vol [1.23410e-04] ncalls [46952] mean calls [2.5000]\n",
      "[nnest.sampler] [INFO] Step [18400] loglstar [-9.9074e+02] max logl [-1.0483e+02] logz [-1.0063e+03] vol [1.01039e-04] ncalls [47590] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [18800] loglstar [-9.0321e+02] max logl [-1.0483e+02] logz [-9.1920e+02] vol [8.27241e-05] ncalls [48298] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [19200] loglstar [-8.2305e+02] max logl [-7.7021e+01] logz [-8.3799e+02] vol [6.77287e-05] ncalls [49093] mean calls [1.8000]\n",
      "[nnest.sampler] [INFO] Step [19600] loglstar [-7.5312e+02] max logl [-7.7021e+01] logz [-7.6890e+02] vol [5.54516e-05] ncalls [50060] mean calls [3.3000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0219]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0607] validation loss [-0.0315]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0644] validation loss [-0.0326]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0653] validation loss [-0.0327]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.0659] validation loss [-0.0328]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.0660] validation loss [-0.0328]\n",
      "[nnest.trainer] [INFO] Epoch [231] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [181] validation loss [-0.0329]\n",
      "[nnest.sampler] [INFO] Step [20000] loglstar [-6.9252e+02] max logl [-7.7021e+01] logz [-7.0797e+02] vol [4.53999e-05] ncalls [51203] mean calls [2.5000]\n",
      "[nnest.sampler] [INFO] Step [20400] loglstar [-6.3291e+02] max logl [-7.7021e+01] logz [-6.4854e+02] vol [3.71703e-05] ncalls [51802] mean calls [1.2000]\n",
      "[nnest.sampler] [INFO] Step [20800] loglstar [-5.8217e+02] max logl [-7.7021e+01] logz [-5.9777e+02] vol [3.04325e-05] ncalls [52561] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [21200] loglstar [-5.3701e+02] max logl [-7.3681e+01] logz [-5.5292e+02] vol [2.49160e-05] ncalls [53424] mean calls [2.0000]\n",
      "[nnest.sampler] [INFO] Step [21600] loglstar [-4.9178e+02] max logl [-7.3681e+01] logz [-5.0800e+02] vol [2.03995e-05] ncalls [54472] mean calls [2.9000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0190]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0747] validation loss [-0.0394]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0791] validation loss [-0.0405]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0796] validation loss [-0.0406]\n",
      "[nnest.trainer] [INFO] Epoch [109] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [59] validation loss [-0.0406]\n",
      "[nnest.sampler] [INFO] Step [22000] loglstar [-4.5424e+02] max logl [-7.3681e+01] logz [-4.7051e+02] vol [1.67017e-05] ncalls [55595] mean calls [2.6000]\n",
      "[nnest.sampler] [INFO] Step [22400] loglstar [-4.2007e+02] max logl [-7.0907e+01] logz [-4.3626e+02] vol [1.36742e-05] ncalls [56264] mean calls [1.8000]\n",
      "[nnest.sampler] [INFO] Step [22800] loglstar [-3.8919e+02] max logl [-7.0907e+01] logz [-4.0539e+02] vol [1.11955e-05] ncalls [56998] mean calls [2.6000]\n",
      "[nnest.sampler] [INFO] Step [23200] loglstar [-3.5993e+02] max logl [-7.0907e+01] logz [-3.7619e+02] vol [9.16609e-06] ncalls [57882] mean calls [2.3000]\n",
      "[nnest.sampler] [INFO] Step [23600] loglstar [-3.3561e+02] max logl [-7.0907e+01] logz [-3.5228e+02] vol [7.50456e-06] ncalls [58930] mean calls [2.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0166]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.0880] validation loss [-0.0455]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.0927] validation loss [-0.0466]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.0934] validation loss [-0.0466]\n",
      "[nnest.trainer] [INFO] Epoch [148] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [98] validation loss [-0.0467]\n",
      "[nnest.sampler] [INFO] Step [24000] loglstar [-3.1312e+02] max logl [-7.0907e+01] logz [-3.2980e+02] vol [6.14421e-06] ncalls [60122] mean calls [3.0000]\n",
      "[nnest.sampler] [INFO] Step [24400] loglstar [-2.9182e+02] max logl [-6.3909e+01] logz [-3.0861e+02] vol [5.03046e-06] ncalls [60762] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [24800] loglstar [-2.7298e+02] max logl [-5.4018e+01] logz [-2.8992e+02] vol [4.11859e-06] ncalls [61487] mean calls [1.3000]\n",
      "[nnest.sampler] [INFO] Step [25200] loglstar [-2.5146e+02] max logl [-3.0757e+01] logz [-2.6856e+02] vol [3.37202e-06] ncalls [62299] mean calls [3.1000]\n",
      "[nnest.sampler] [INFO] Step [25600] loglstar [-2.3377e+02] max logl [-3.0757e+01] logz [-2.5085e+02] vol [2.76077e-06] ncalls [63350] mean calls [1.7000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0143]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1024] validation loss [-0.0530]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1071] validation loss [-0.0539]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1078] validation loss [-0.0539]\n",
      "[nnest.trainer] [INFO] Epoch [109] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [59] validation loss [-0.0540]\n",
      "[nnest.sampler] [INFO] Step [26000] loglstar [-2.1841e+02] max logl [-3.0757e+01] logz [-2.3552e+02] vol [2.26033e-06] ncalls [64485] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [26400] loglstar [-2.0429e+02] max logl [-3.0757e+01] logz [-2.2168e+02] vol [1.85060e-06] ncalls [65152] mean calls [1.8000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.sampler] [INFO] Step [26800] loglstar [-1.9011e+02] max logl [-3.0757e+01] logz [-2.0774e+02] vol [1.51514e-06] ncalls [65886] mean calls [1.4000]\n",
      "[nnest.sampler] [INFO] Step [27200] loglstar [-1.7756e+02] max logl [-3.0757e+01] logz [-1.9534e+02] vol [1.24050e-06] ncalls [66757] mean calls [3.1000]\n",
      "[nnest.sampler] [INFO] Step [27600] loglstar [-1.6619e+02] max logl [-3.0757e+01] logz [-1.8396e+02] vol [1.01563e-06] ncalls [67796] mean calls [2.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0124]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1168] validation loss [-0.0597]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1215] validation loss [-0.0607]\n",
      "[nnest.trainer] [INFO] Epoch [63] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [13] validation loss [-0.0608]\n",
      "[nnest.sampler] [INFO] Step [28000] loglstar [-1.5536e+02] max logl [-3.0757e+01] logz [-1.7322e+02] vol [8.31529e-07] ncalls [68916] mean calls [5.4000]\n",
      "[nnest.sampler] [INFO] Step [28400] loglstar [-1.4515e+02] max logl [-3.0757e+01] logz [-1.6335e+02] vol [6.80798e-07] ncalls [69606] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [28800] loglstar [-1.3618e+02] max logl [-3.0757e+01] logz [-1.5428e+02] vol [5.57390e-07] ncalls [70392] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [29200] loglstar [-1.2777e+02] max logl [-2.9392e+01] logz [-1.4605e+02] vol [4.56353e-07] ncalls [71254] mean calls [2.8000]\n",
      "[nnest.sampler] [INFO] Step [29600] loglstar [-1.2045e+02] max logl [-2.9392e+01] logz [-1.3867e+02] vol [3.73630e-07] ncalls [72351] mean calls [2.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0106]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1313] validation loss [-0.0679]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1359] validation loss [-0.0696]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1364] validation loss [-0.0698]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.1373] validation loss [-0.0698]\n",
      "[nnest.trainer] [INFO] Epoch [169] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [119] validation loss [-0.0698]\n",
      "[nnest.sampler] [INFO] Step [30000] loglstar [-1.1395e+02] max logl [-2.9392e+01] logz [-1.3245e+02] vol [3.05902e-07] ncalls [73587] mean calls [3.2000]\n",
      "[nnest.sampler] [INFO] Step [30400] loglstar [-1.0751e+02] max logl [-2.8561e+01] logz [-1.2613e+02] vol [2.50452e-07] ncalls [74238] mean calls [1.3000]\n",
      "[nnest.sampler] [INFO] Step [30800] loglstar [-1.0113e+02] max logl [-2.4359e+01] logz [-1.1996e+02] vol [2.05052e-07] ncalls [74929] mean calls [2.3000]\n",
      "[nnest.sampler] [INFO] Step [31200] loglstar [-9.5124e+01] max logl [-2.4359e+01] logz [-1.1410e+02] vol [1.67883e-07] ncalls [75749] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [31600] loglstar [-8.9657e+01] max logl [-2.3680e+01] logz [-1.0863e+02] vol [1.37451e-07] ncalls [76766] mean calls [2.3000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0092]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1460] validation loss [-0.0750]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1506] validation loss [-0.0758]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1510] validation loss [-0.0757]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.1514] validation loss [-0.0758]\n",
      "[nnest.trainer] [INFO] Epoch [172] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [122] validation loss [-0.0760]\n",
      "[nnest.sampler] [INFO] Step [32000] loglstar [-8.5158e+01] max logl [-2.3680e+01] logz [-1.0411e+02] vol [1.12535e-07] ncalls [77941] mean calls [2.8000]\n",
      "[nnest.sampler] [INFO] Step [32400] loglstar [-8.0386e+01] max logl [-1.6140e+01] logz [-9.9647e+01] vol [9.21360e-08] ncalls [78572] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [32800] loglstar [-7.6235e+01] max logl [-1.6140e+01] logz [-9.5534e+01] vol [7.54346e-08] ncalls [79298] mean calls [1.9000]\n",
      "[nnest.sampler] [INFO] Step [33200] loglstar [-7.1853e+01] max logl [-1.6140e+01] logz [-9.1436e+01] vol [6.17606e-08] ncalls [80142] mean calls [2.9000]\n",
      "[nnest.sampler] [INFO] Step [33600] loglstar [-6.7744e+01] max logl [-1.6140e+01] logz [-8.7481e+01] vol [5.05653e-08] ncalls [81108] mean calls [2.3000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0080]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1610] validation loss [-0.0816]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1649] validation loss [-0.0822]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1657] validation loss [-0.0822]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.1664] validation loss [-0.0822]\n",
      "[nnest.trainer] [INFO] Epoch [176] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [126] validation loss [-0.0823]\n",
      "[nnest.sampler] [INFO] Step [34000] loglstar [-6.4000e+01] max logl [-1.6140e+01] logz [-8.3891e+01] vol [4.13994e-08] ncalls [82285] mean calls [3.2000]\n",
      "[nnest.sampler] [INFO] Step [34400] loglstar [-6.0936e+01] max logl [-1.6140e+01] logz [-8.0755e+01] vol [3.38949e-08] ncalls [82916] mean calls [1.4000]\n",
      "[nnest.sampler] [INFO] Step [34800] loglstar [-5.7800e+01] max logl [-1.6140e+01] logz [-7.7843e+01] vol [2.77508e-08] ncalls [83626] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [35200] loglstar [-5.4794e+01] max logl [-1.6140e+01] logz [-7.5013e+01] vol [2.27205e-08] ncalls [84402] mean calls [2.0000]\n",
      "[nnest.sampler] [INFO] Step [35600] loglstar [-5.1804e+01] max logl [-1.1074e+01] logz [-7.2169e+01] vol [1.86019e-08] ncalls [85399] mean calls [3.0000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0070]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1749] validation loss [-0.0904]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1787] validation loss [-0.0912]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1795] validation loss [-0.0913]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.1800] validation loss [-0.0913]\n",
      "[nnest.trainer] [INFO] Epoch [173] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [123] validation loss [-0.0914]\n",
      "[nnest.sampler] [INFO] Step [36000] loglstar [-4.9429e+01] max logl [-1.1074e+01] logz [-6.9793e+01] vol [1.52300e-08] ncalls [86530] mean calls [3.1000]\n",
      "[nnest.sampler] [INFO] Step [36400] loglstar [-4.7062e+01] max logl [-1.1074e+01] logz [-6.7681e+01] vol [1.24693e-08] ncalls [87138] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [36800] loglstar [-4.4631e+01] max logl [-1.1074e+01] logz [-6.5424e+01] vol [1.02090e-08] ncalls [87838] mean calls [1.8000]\n",
      "[nnest.sampler] [INFO] Step [37200] loglstar [-4.2366e+01] max logl [-1.1074e+01] logz [-6.3310e+01] vol [8.35839e-09] ncalls [88602] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [37600] loglstar [-4.0432e+01] max logl [-9.1441e+00] logz [-6.1450e+01] vol [6.84327e-09] ncalls [89556] mean calls [3.3000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0060]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.1893] validation loss [-0.0964]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.1933] validation loss [-0.0972]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.1939] validation loss [-0.0972]\n",
      "[nnest.trainer] [INFO] Epoch [141] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [91] validation loss [-0.0973]\n",
      "[nnest.sampler] [INFO] Step [38000] loglstar [-3.8460e+01] max logl [-9.1441e+00] logz [-5.9642e+01] vol [5.60280e-09] ncalls [90764] mean calls [2.8000]\n",
      "[nnest.sampler] [INFO] Step [38400] loglstar [-3.6746e+01] max logl [-9.1441e+00] logz [-5.7967e+01] vol [4.58718e-09] ncalls [91395] mean calls [1.5000]\n",
      "[nnest.sampler] [INFO] Step [38800] loglstar [-3.5064e+01] max logl [-9.1441e+00] logz [-5.6507e+01] vol [3.75567e-09] ncalls [92103] mean calls [1.3000]\n",
      "[nnest.sampler] [INFO] Step [39200] loglstar [-3.3411e+01] max logl [-9.1441e+00] logz [-5.4954e+01] vol [3.07488e-09] ncalls [92904] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [39600] loglstar [-3.2053e+01] max logl [-9.1441e+00] logz [-5.3657e+01] vol [2.51750e-09] ncalls [93803] mean calls [2.2000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nnest.trainer] [INFO] Training jitter [0.0053]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.2031] validation loss [-0.1036]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.2068] validation loss [-0.1041]\n",
      "[nnest.trainer] [INFO] Epoch [58] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [8] validation loss [-0.1042]\n",
      "[nnest.sampler] [INFO] Step [40000] loglstar [-3.0773e+01] max logl [-9.1441e+00] logz [-5.2480e+01] vol [2.06115e-09] ncalls [94964] mean calls [3.9000]\n",
      "[nnest.sampler] [INFO] Step [40400] loglstar [-2.9390e+01] max logl [-9.1441e+00] logz [-5.1360e+01] vol [1.68753e-09] ncalls [95701] mean calls [2.0000]\n",
      "[nnest.sampler] [INFO] Step [40800] loglstar [-2.8203e+01] max logl [-9.1441e+00] logz [-5.0257e+01] vol [1.38163e-09] ncalls [96556] mean calls [2.0000]\n",
      "[nnest.sampler] [INFO] Step [41200] loglstar [-2.7166e+01] max logl [-9.1441e+00] logz [-4.9286e+01] vol [1.13119e-09] ncalls [97441] mean calls [2.8000]\n",
      "[nnest.sampler] [INFO] Step [41600] loglstar [-2.6115e+01] max logl [-9.1441e+00] logz [-4.8389e+01] vol [9.26136e-10] ncalls [98443] mean calls [2.8000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0046]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.2154] validation loss [-0.1097]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.2199] validation loss [-0.1106]\n",
      "[nnest.trainer] [INFO] Epoch [83] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [33] validation loss [-0.1109]\n",
      "[nnest.sampler] [INFO] Step [42000] loglstar [-2.5198e+01] max logl [-9.1441e+00] logz [-4.7562e+01] vol [7.58256e-10] ncalls [99693] mean calls [3.0000]\n",
      "[nnest.sampler] [INFO] Step [42400] loglstar [-2.4249e+01] max logl [-9.1441e+00] logz [-4.6783e+01] vol [6.20808e-10] ncalls [100392] mean calls [3.1000]\n",
      "[nnest.sampler] [INFO] Step [42800] loglstar [-2.3336e+01] max logl [-9.1441e+00] logz [-4.6034e+01] vol [5.08274e-10] ncalls [101146] mean calls [1.7000]\n",
      "[nnest.sampler] [INFO] Step [43200] loglstar [-2.2486e+01] max logl [-9.1441e+00] logz [-4.5316e+01] vol [4.16140e-10] ncalls [101999] mean calls [2.7000]\n",
      "[nnest.sampler] [INFO] Step [43600] loglstar [-2.1614e+01] max logl [-9.1441e+00] logz [-4.4639e+01] vol [3.40706e-10] ncalls [103068] mean calls [2.6000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0040]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.2293] validation loss [-0.1167]\n",
      "[nnest.trainer] [INFO] Epoch [50] train loss [-0.2335] validation loss [-0.1175]\n",
      "[nnest.trainer] [INFO] Epoch [100] train loss [-0.2339] validation loss [-0.1175]\n",
      "[nnest.trainer] [INFO] Epoch [150] train loss [-0.2344] validation loss [-0.1177]\n",
      "[nnest.trainer] [INFO] Epoch [200] train loss [-0.2344] validation loss [-0.1176]\n",
      "[nnest.trainer] [INFO] Epoch [250] train loss [-0.2351] validation loss [-0.1177]\n",
      "[nnest.trainer] [INFO] Epoch [300] train loss [-0.2351] validation loss [-0.1176]\n",
      "[nnest.trainer] [INFO] Epoch [350] train loss [-0.2352] validation loss [-0.1178]\n",
      "[nnest.trainer] [INFO] Epoch [400] train loss [-0.2353] validation loss [-0.1178]\n",
      "[nnest.trainer] [INFO] Epoch [404] ran out of patience\n",
      "[nnest.trainer] [INFO] Best epoch [354] validation loss [-0.1180]\n",
      "[nnest.sampler] [INFO] Step [44000] loglstar [-2.0855e+01] max logl [-9.0116e+00] logz [-4.3994e+01] vol [2.78947e-10] ncalls [104363] mean calls [5.7000]\n",
      "[nnest.sampler] [INFO] Step [44400] loglstar [-2.0229e+01] max logl [-9.0116e+00] logz [-4.3419e+01] vol [2.28382e-10] ncalls [105024] mean calls [1.3000]\n",
      "[nnest.sampler] [INFO] Step [44800] loglstar [-1.9484e+01] max logl [-9.0116e+00] logz [-4.2883e+01] vol [1.86984e-10] ncalls [105725] mean calls [2.6000]\n",
      "[nnest.sampler] [INFO] Step [45200] loglstar [-1.8839e+01] max logl [-9.0116e+00] logz [-4.2360e+01] vol [1.53089e-10] ncalls [106506] mean calls [2.1000]\n",
      "[nnest.sampler] [INFO] Step [45600] loglstar [-1.8250e+01] max logl [-9.0116e+00] logz [-4.1877e+01] vol [1.25339e-10] ncalls [107524] mean calls [2.8000]\n",
      "[nnest.trainer] [INFO] Number of training samples [2000]\n",
      "[nnest.trainer] [INFO] Training jitter [0.0035]\n",
      "[nnest.trainer] [INFO] Epoch [1] train loss [-0.2434] validation loss [-0.1244]\n"
     ]
    }
   ],
   "source": [
    "sampler.run(strategy=['rejection_prior', 'density_flow'], volume_switch=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampler.logz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MCSamples(samples=sampler.samples, weights=sampler.weights, loglikes=-sampler.loglikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mc.getEffectiveSamples())\n",
    "print(mc.getMargeStats())\n",
    "print(mc.likeStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plots.getSubplotPlotter(width_inch=8)\n",
    "g.triangle_plot(mc, filled=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
